{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Linear Regression\n",
      "LR.a = 0.25\n",
      "LR.b = 1.5\n",
      "LR.c = 0.5399999999999997\n",
      "LR.d = 0.06000000000000019\n",
      "Based on the above values, the best Linear Regression is LR.d (y=.8*x+.7)\n",
      "\n",
      "Question 2: Lasso and Ridge Regression\n",
      "The problem solved by Lasso and Ridge Regression is that they can make tighter models and prevent overfitting in linear regression\n",
      "The major difference between Ridge and Lasso is that Ridge squares the variables and reduces importance of some features, whereas Lasso just uses the absolute value of the variable and makes the weight zero.\n",
      "Some advantages of Lasso regression are its less likely to overfit, its easier to interpret, and reduces variance in models\n",
      "Disadvantages of Lasso regression are bad with larger models (more predictors)\n",
      "Some advantages of Ridge regression are that it reduces the model complexity, it is less sensitive to weight, better long term predictions \n",
      "Some disadvantages of Ridge regression are that it is prone to overfitting, drops variance\n",
      "\n",
      "Question 3: Decision Tree\n",
      "Node Impurities: \n",
      "Gini Index: a-left: 0.375 ; right:  0.10493827160493831 ; b-left:  0.4444444444444445 ; right:  0.4444444444444445 ; c-left:  0.17233560090702943 ; right:  0.19753086419753085 ;\n",
      "Entropy: a-left: 0.8112781244591328 ; right: 0.3095434291503252 ; b-left: 0.9182958340544896 ; right: 0.9182958340544896 ; c-left: 0.4537163391869448 ; right: 0.5032583347756457 ;\n",
      "Misclass Error: a-left: 0.25 ; right: 0.05555555555555558 ; b-left: 0.33333333333333337 ; right 0.33333333333333337 ; c-left: 0.09523809523809523 ; right: 0.11111111111111116 ;\n",
      "Qualities of splitting: \n",
      "Gini Index: a: 0.21200000000000002 ; b: 0.44 ; c: 0.179 ;\n",
      "Entropy: a: 0.40800000000000003 ; b: 0.0 ; c: 0.45300000000000007 ;\n",
      "Misclass Error: a: 0.19733333333333336 ; b: 0.003333333333333355 ; c: 0.2303333333333334 ;\n",
      "3.c-Based on the values, the best split for Gini index is , Entropy is , and Misclass is .\n",
      "3.d-All three methonds have the best splitting.\n",
      "\n",
      "Question 4: kNN\n",
      "Class Iris-versicolor ID => 0\n",
      "Class Iris-setosa ID => 1\n",
      "Class Iris-virginica ID => 2\n",
      "Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]\n",
      "Mean Accuracy: 96.667%\n",
      "\n",
      "\n",
      "Question 5: Pruning\n",
      "The error before splitting is:  0.35\n",
      "The error after splitting is:  11.0\n",
      "Based on the above values, the tree should be pruned, error increases after split.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import math\n",
    "import pdb\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "##%% Question 1: Linear regression\n",
    "\n",
    "#points (0.5, 1), (2, 2.5), and (3, 3)\n",
    "x = np.array([.5, 2, 3])\n",
    "y = np.array([1, 2.5, 3])\n",
    "\n",
    "yn = 0\n",
    "functa = 0\n",
    "functb = 0\n",
    "functc = 0\n",
    "functd = 0\n",
    "\n",
    "#for a\n",
    "for i in range(len(x)):\n",
    "    yn = x[i] + .5\n",
    "    a = (y[i] - yn)**2\n",
    "    functa += a\n",
    "    \n",
    "#for b\n",
    "for i in range(len(x)):\n",
    "    yn = x[i] + 1\n",
    "    b = (y[i] - yn)**2\n",
    "    functb += b\n",
    "    \n",
    "#for c\n",
    "for i in range(len(x)):\n",
    "    yn = x[i] * .8 + .3\n",
    "    c = (y[i] - yn)**2\n",
    "    functc += c\n",
    "    \n",
    "#for d\n",
    "for i in range(len(x)):\n",
    "    yn = x[i] * .8 + .7\n",
    "    d = (y[i] - yn)**2\n",
    "    functd += d\n",
    "    \n",
    "print(\"Question 1: Linear Regression\")\n",
    "print(\"LR.a =\", functa)\n",
    "print(\"LR.b =\", functb)\n",
    "print(\"LR.c =\", functc)\n",
    "print(\"LR.d =\", functd)\n",
    "print(\"Based on the above values, the best Linear Regression is LR.d (y=.8*x+.7)\\n\")\n",
    "\n",
    "##%% Question 2: Lasso and Ridge regression\n",
    "print(\"Question 2: Lasso and Ridge Regression\")\n",
    "print(\"The problem solved by Lasso and Ridge Regression is that they can make tighter models and prevent overfitting in linear regression\")\n",
    "print(\"The major difference between Ridge and Lasso is that Ridge squares the variables and reduces importance of some features, whereas Lasso just uses the absolute value of the variable and makes the weight zero.\")\n",
    "print(\"Some advantages of Lasso regression are its less likely to overfit, its easier to interpret, and reduces variance in models\")\n",
    "print(\"Disadvantages of Lasso regression are bad with larger models (more predictors)\")\n",
    "print(\"Some advantages of Ridge regression are that it reduces the model complexity, it is less sensitive to weight, better long term predictions \")\n",
    "print(\"Some disadvantages of Ridge regression are that it is prone to overfitting, drops variance\\n\")\n",
    "\n",
    "##%% Question 3: Decision tree\n",
    "#gini formula 1-(3/12)**2-(9/12)**2\n",
    "#Entropy formula -(3/12)*math.log2(3/12)-(9/12)*math.log2(9/12)\n",
    "#misclass formula 1- max(3/12, 9/12)\n",
    "\n",
    "#values are saved for reference on the pdf table\n",
    "\n",
    "#entropy of the parents is .918\n",
    "#-(20/30)*math.log2(20/30)-(10/30)*math.log2(10/30)\n",
    "\n",
    "print(\"Question 3: Decision Tree\")\n",
    "print(\"Node Impurities: \")\n",
    "print(\"Gini Index: a-left:\", 1-(3/12)**2-(9/12)**2, \"; right: \", 1-(17/18)**2-(1/18)**2, \"; b-left: \", 1-(10/15)**2-(5/15)**2, \"; right: \", 1-(10/15)**2-(5/15)**2, \"; c-left: \", 1-(19/21)**2-(2/21)**2, \"; right: \", 1-(1/9)**2-(8/9)**2, \";\") \n",
    "print(\"Entropy: a-left:\",-(3/12)*math.log2(3/12)-(9/12)*math.log2(9/12),\"; right:\",-(17/18)*math.log2(17/18)-(1/18)*math.log2(1/18),\"; b-left:\",-(10/15)*math.log2(10/15)-(5/15)*math.log2(5/15),\"; right:\",-(10/15)*math.log2(10/15)-(5/15)*math.log2(5/15),\"; c-left:\",-(19/21)*math.log2(19/21)-(2/21)*math.log2(2/21),\"; right:\",-(1/9)*math.log2(1/9)-(8/9)*math.log2(8/9),\";\")\n",
    "print(\"Misclass Error: a-left:\",1-max(3/12, 9/12),\"; right:\",1-max(17/18, 1/18),\"; b-left:\",1-max(10/15, 5/15),\"; right\",1-max(10/15, 5/15),\"; c-left:\",1-max(19/21, 2/21),\"; right:\",1-max(1/9, 8/9),\";\")\n",
    "\n",
    "print(\"Qualities of splitting: \")\n",
    "print(\"Gini Index: a:\",(12/30) * .38 + (18/30) * .10,\"; b:\",(15/30) * .44 + (15/30) * .44,\"; c:\",(21/30) * .17 + (9/30) * .20,\";\")\n",
    "print(\"Entropy: a:\",0.918 - ((12/30) * .81 + (18/30) * .31),\"; b:\",0.918 - ((15/30) * .918 + (15/30) * .918),\"; c:\",0.918 - ((21/30) * .45 + (9/30) * .50),\";\")\n",
    "print(\"Misclass Error: a:\",1 - max(20/30, 10/30) - (12/30*.25 + 18/30*.06),\"; b:\", 1 - max(20/30, 10/30) -(15/30*.33 + 15/30 * .33),\"; c:\", 1 - max(20/30, 10/30) - (21/30*.10 + 9/30 * .11),\";\")\n",
    "\n",
    "print(\"3.c-Based on the values, the best split for Gini index is , Entropy is , and Misclass is .\")\n",
    "print(\"3.d-All three methonds have the best splitting.\\n\")\n",
    "\n",
    "##%% Question 4: KNN\n",
    "print(\"Question 4: kNN\")\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('Class %s ID => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\n",
    "    distance = 0.0\n",
    "    ### your code starts\n",
    "    for i in range(len(row1) - 1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    ### your code ends\n",
    "    return sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors and return the list of neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "    ### your code starts\n",
    "        distances.append((train_row, euclidean_distance(test_row, train_row)))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    ### your code ends\n",
    "    \n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "      neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    ### your code starts\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    # get all neighbors and make the prediction based on majority of neighbors\n",
    "    values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(values), key=values.count)\n",
    "    ### your code ends\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# the output is\n",
    "\n",
    "#The output scores is 96.66666666666667\n",
    "#The mean accuracy is 96.667%\n",
    "\n",
    "##%% Question 5: Pruning of decision tree\n",
    "#error pre split\n",
    "print(\"\\n\")\n",
    "print(\"Question 5: Pruning\")\n",
    "pre_split = (10 + 0.5)/30\n",
    "print(\"The error before splitting is: \", pre_split)\n",
    "\n",
    "#post split\n",
    "post_split = (9 + 4*0.5)\n",
    "print(\"The error after splitting is: \", post_split)\n",
    "\n",
    "print(\"Based on the above values, the tree should be pruned, error increases after split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
